{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCESS TO S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.14 64-bit' requires notebook and jupyter package.\n",
      "Run the following command to install 'jupyter and notebook' into the Python environment. \n",
      "Command: 'python -m pip install jupyter notebook -U\n",
      "or\n",
      "conda install jupyter notebook -U'\n",
      "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "contents = conn.list_objects(Bucket=bucket_name)['Contents']\n",
    "for f in contents:\n",
    "    print(f['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.9.14 64-bit' requires jupyter and notebook package.\n",
      "Run the following command to install 'jupyter and notebook' into the Python environment. \n",
      "Command: 'python -m pip install jupyter notebook -U\n",
      "or\n",
      "conda install jupyter notebook -U'\n",
      "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# The unzipping function \n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_func(bucket_zipfile_path):\n",
    "    \"\"\"Unzips a file. \n",
    "    Parameters:\n",
    "    bucket_zipfile_path : str, path to the zipfile in the bucket\n",
    "    \"\"\"\n",
    "    # Set up connection\n",
    "    s3_resource = boto3.client('s3')\n",
    "    path_bucket = \"bucket_name\" # Change it to your bucket name\n",
    "    path_to_move_file = \"destination_subfolder/\" # Change it to your \n",
    "                                                 # destination subfolder\n",
    "    \n",
    "    # Create local targets\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    os.makedirs(\"./unzipped_data\", exist_ok=True)\n",
    "    \n",
    "    # Download zipfile\n",
    "    boto3.resource('s3').Object(path_bucket, bucket_zipfile_path).\\\n",
    "                        download_file(Filename=\"./data/zipfile.zip\")\n",
    "    \n",
    "        # Unzip\n",
    "    for zip in os.listdir(\"./data\"):\n",
    "        with zipfile.ZipFile(os.path.join(\"./data\", zip), 'r') as file:\n",
    "            file.extractall(\"./unzipped_data\")\n",
    "            \n",
    "    # Upload\n",
    "    for file in os.listdir(\"./unzipped_data\"):\n",
    "        output_path = path_to_move_file + file\n",
    "        s3_resource.upload_file(os.path.join(\"./unzipped_data\", file), \n",
    "                                path_bucket, output_path)\n",
    "        \n",
    "    # Return S3 path to last extracted csv file\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader function \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "def csv_s3_reader(bucket_name, \n",
    "                  csv_path, \n",
    "                  sep=\";\", \n",
    "                  decimal=\",\", \n",
    "                  encoding=\"utf-8\"):\n",
    "  \"\"\"Reads a csv file from S3.\n",
    "  Parameters:\n",
    "  bucket_name : str, S3 bucket name,\n",
    "  csv_path : str, path to your csv file in your bucket\n",
    "  sep : str, parameter for pd.read_csv(),\n",
    "  decimal : str, parameter for pd.read_csv(),\n",
    "  encoding : str, parameter for pd.read_csv()\n",
    "  \n",
    "  Change sep, decimal and encoding depending on your csv's formatting\n",
    "  \"\"\"\n",
    "  \n",
    "  # Set up connection and download file\n",
    "  csv_obj = boto3.client('s3').get_object(Bucket=bucket_name, Key=csv_path)\n",
    "  body = csv_obj['Body']\n",
    "  csv_string = body.read().decode(encoding)\n",
    "  \n",
    "  # Read csv\n",
    "  df = pd.read_csv(StringIO(csv_string), sep=sep, decimal=decimal, error_bad_lines=False, encoding=encoding)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat python functions to component functions to be  understandable by Kubeflow\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "\n",
    "# First function\n",
    "def unzip_files(bucket_zipfile_path : str, \n",
    "                output_string : comp.OutputPath(str)) :\n",
    "    \n",
    "    import boto3\n",
    "    import os\n",
    "    from io import BytesIO\n",
    "    import zipfile\n",
    " \n",
    "    # Set up connection\n",
    "    s3_resource = boto3.client('s3')\n",
    "    path_bucket = \"bucket_name\" # Change it to your bucket name\n",
    "    path_to_move_file = \"destination_subfolder/\" # Change it to your \n",
    "                                                 # destination subfolder\n",
    "    \n",
    "    # Create local targets\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    os.makedirs(\"./unzipped_data\", exist_ok=True)\n",
    "    \n",
    "    # Download zipfile\n",
    "    boto3.resource('s3').Object(path_bucket, bucket_zipfile_path).\\\n",
    "                        download_file(Filename=\"./data/zipfile.zip\")\n",
    "    # Unzip\n",
    "    for zip in os.listdir(\"./data\"):\n",
    "        with zipfile.ZipFile(os.path.join(\"./data\", zip), 'r') as file:\n",
    "            file.extractall(\"./unzipped_data\")\n",
    "    \n",
    "    # Upload\n",
    "    for file in os.listdir(\"./unzipped_data\"):\n",
    "        output_path = path_to_move_file + file\n",
    "        s3_resource.upload_file(os.path.join(\"./unzipped_data\", file), \n",
    "                                path_bucket, output_path)\n",
    "    \n",
    "    # Create Artifact : S3 path to last extracted csv file    \n",
    "    with open(output_string, 'w') as writer:\n",
    "        writer.write(output_path)\n",
    "        \n",
    "# 2nd function\n",
    "def csv_s3_reader(bucket_name : str, \n",
    "                  csv_path : comp.InputPath(str), \n",
    "                  sep : str, \n",
    "                  decimal : str, \n",
    "                  encoding : str, \n",
    "                  output_csv:comp.OutputPath('CSV')):\n",
    "    \n",
    "\n",
    "    from io import StringIO\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import os\n",
    "     # Set up connection and download file\n",
    "    with open(csv_path, 'r') as reader:\n",
    "        line = reader.readline()\n",
    "        csv_obj = boto3.client('s3').get_object(Bucket=bucket_name, Key=line)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode(encoding)\n",
    "    \n",
    "    # Read csv\n",
    "    df = pd.read_csv(StringIO(csv_string), sep=sep, decimal=decimal, \n",
    "                     error_bad_lines=False, encoding=encoding)\n",
    "    \n",
    "    df.to_csv(output_csv, index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
